{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1843d57-fc22-4081-a72a-a9409eca7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pyspark import sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465c6a1b-3feb-4fb1-b9d2-bdc13df60d4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "('An error occurred while calling o37.jdbc.\\n', JavaObject id=o39)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 4, in <module>",
      "  File \"/home/ubuntu/spark/python/pyspark/sql/readwriter.py\", line 927, in jdbc\n    return self._df(self._jreader.jdbc(url, table, jprop))",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(",
      "  File \"/home/ubuntu/spark/python/pyspark/errors/exceptions/captured.py\", line 169, in deco\n    return f(*a, **kw)",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(",
      "Py4JJavaError: An error occurred while calling o37.jdbc.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:41)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ReadMySQL\").getOrCreate()\n",
    "url = \"jdbc:mysql://zero.cqjpoyzx2xld.ap-northeast-2.rds.amazonaws.com:3306/zero\"\n",
    "properties ={\"user\": \"zero\", \"password\": \"tkrkwl1564!\"}\n",
    "df = spark.read.jdbc(url=url, table=\"in_list\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7535fc-38de-4096-b6fe-faabff64f261",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "(\"name 'mysql' is not defined\",)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"python cell\", line 6, in <module>",
      "NameError: name 'mysql' is not defined"
     ]
    }
   ],
   "source": [
    "appName = \"PySpark MySQL Example - via mysql.connector\"\n",
    "master = \"local\"\n",
    "spark = SparkSession.builder.master(master).appName(appName).getOrCreate()\n",
    "\n",
    "# Establish a connection\n",
    "conn = mysql.connector.connect(user='zero', database='zero', password='tkrkwl1564!', host='zero.cqjpoyzx2xld.ap-northeast-2.rds.amazonaws.com:3306')\n",
    "\n",
    "# Read data from MySQL table into a dataframe\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost/test_db\").option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"test_table\").option(\"user\", \"hive\").option(\"password\", \"hive\").load()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "file_extension": ".py",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
