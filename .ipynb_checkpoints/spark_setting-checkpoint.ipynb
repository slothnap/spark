{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b2e9e9-a005-4474-a31d-c19ab8ca0cbb",
   "metadata": {},
   "source": [
    "참고: https://bab-dev-study.tistory.com/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690cdad-e380-4979-af2c-9cfe93de8d13",
   "metadata": {},
   "source": [
    "* 경로 \n",
    "\t1. mysql\n",
    "\t\t- 테이블 준비\n",
    "\t2. jupyter\n",
    "\t\t- Spark\n",
    "\t\t\t- Ubuntu 기반 java 설치 필수\n",
    "\t\t\t\t- Spark는 자바기반으로 작동하여 \n",
    "\t\t\t\t- .bash_profile 설정 필요 \n",
    "\t\t\t- 하둡 + Spark 라이브러리 설치 필요 \n",
    "\t\t\t\t- Spark 사용은 하둡이 필요함\n",
    "\t\t\t\t- \"spark-2.4.8-bin-hadoop2.6.tgz\"\n",
    "\t\t- PySpark\n",
    "\t\t\t- jupyter에 PySpark 커널 생성\n",
    "\t\t\t\t- PySpark 커널 만들면 Spark 커널도 자동으로 생성\n",
    "\t\t\t- PySpark 원리\n",
    "\t\t\t\t- python 기반 Spark 작성 -> Spark로 변환(스칼라) \n",
    "\t\t\t- PySpark 사용법\n",
    "\t\t\t\t- PySpark 구문으로 read jdbc \n",
    "\t3. s3 \n",
    "\n",
    "-------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------\n",
    "* 특정 파일 찾기 \n",
    "\t- $ find . -name \"*파일명*\"\n",
    "\n",
    "* 특정 파일 삭제\n",
    "\t- $ rm 파일명\n",
    "\n",
    "* 특정 디렉토리 삭제 \n",
    "\t- $ rm -r 디렉토리명\n",
    "\n",
    "* 폴더 및 파일 용량 확인\n",
    "\t- $ du -hs *\n",
    "\n",
    "* vi 편집 오류\n",
    "\t- rm 파일명.swp\n",
    "\n",
    "-------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------\n",
    "### Spark 설치 \n",
    "\n",
    "* java 설치 \n",
    "\t# java 설치\n",
    "\t- $ sudo apt-get update\n",
    "\t- $ sudo apt-get upgrade \n",
    "\t- $ sudo apt-get install openjdk-11-jdk\n",
    "\t\n",
    "\t# java 경로 찾기 \n",
    "\t- $ which java\n",
    "\n",
    "\t# 유닉스 환경에서 로그인하는데 사용하는 환경설정 파일\n",
    "\t- $ vi .profile                              \n",
    "    \n",
    "    # 기본 자바 경로 \n",
    "    - export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))\n",
    "\t- export PATH=$PATH:$JAVA_HOME/bin\n",
    "\t    \n",
    "    # shell 즉시 적용 \n",
    "    - $ source .profile\n",
    "    \n",
    "    # 설정 확인\n",
    "    - $ echo $JAVA_HOME   \n",
    "\n",
    "\n",
    "* spark 설치 \n",
    "\t# 다운로드: https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.6.tgz\n",
    "\t- $ tar -zxf spark-3.4.0-bin-hadoop3.tgz\n",
    "\t- $ mv spark-3.4.0-bin-hadoop3 spark\n",
    "\n",
    "\t# 환경설정 \n",
    "\t- $ vi .profile\n",
    "\t- export SPARK_HOME=/home/ubuntu/spark\n",
    "\n",
    "\t# shell 만들기\n",
    "\t- $ cp spark-defaults.conf.template spark-defaults.conf\n",
    "\t- $ cp spark-env.sh.template spark-env.sh\n",
    "\t- $ cp log4j2.properties.template log4j2.properties\n",
    "\n",
    "\t- $ vi conf/log4j2.properties\n",
    "\t- log4j2.rootCategory=WARN, console\n",
    "\n",
    "\t\n",
    "\t# 설치 확인\n",
    "\t## pyspark 자체를 실행\n",
    "\t- $ ./bin/pyspark\n",
    "\n",
    "\t## Spark 테스트              \n",
    "\t>>> lines =sc.textFile(\"README.md\")\n",
    "\t>>> lines.count()\n",
    "\t>>> lines.first()\n",
    "\t>>> pythonLines = lines.filter(lambda line : \"Python\" in line)\n",
    "\t>>> pythonLines.first()\n",
    "\n",
    "\n",
    "\t# 파일로 실행 \n",
    "\t- $ vi test.py\n",
    "\t\tfrom pyspark import SparkConf, SparkContext\n",
    "\t \n",
    "\t\tconf = SparkConf().setMaster(\"local\").setAppName(\"test\")\n",
    "\t\tsc = SparkContext(conf=conf)\n",
    "\t\t \n",
    "\t\tlines =sc.textFile(\"./README.md\")\n",
    "\t\t \n",
    "\t\tprint(\"lines.count() :\", lines.count() ) \n",
    "\t\tprint(\"lines.first() :\", lines.first() )\n",
    "\t\t \n",
    "\t\tpythonLines = lines.filter(lambda line : \"Python\" in line)\n",
    "\t\tprint(\"pythonLines.first() :\", pythonLines.first())\n",
    "\n",
    "\t- $ ./bin/spark-submit test.py -- 모든 클러스터 매니저 간에 작업을 제출해 주는 단일 툴\n",
    "\n",
    "\n",
    "\t## 스파크 서브밋에서 실행하는 환경\n",
    "\t-- $ spark-shell                \n",
    "\n",
    "\n",
    "* PySpark-kernel 설치 \n",
    "\t-- https://shrimp-taco.tistory.com/entry/Spark-JupyterLab%EC%97%90-pyspark-kernel-%EC%84%A4%EC%B9%98-No-module-named-pysparkkernelinstall-%ED%95%B4%EA%B2%B0\n",
    "\t- pip3 install pyspark_kernel\n",
    "\t- python -m pyspark_kernel.install"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
